Transcript with 1-minute Timeline Markers
=====================================


[0:00:00]
Alright folks, so what we're going to do now is we're going to set up a very simple server and a very simple client to interact with that server. To do that, we're going to be following closely the quick start example that you can find in the server developer section in the model context protocol documentation. I'll leave a link with all the necessary information in the description of this video and in the setup for this video with all the materials and resources you need. Now, to set it up, it's very easy. The first thing we're going to do is make sure that you have UV installed. Now, UV is an amazing Python and project manager tool that allows you to automate a bunch of things that you usually would have to take care of if you're just using PIP. And it's a very cool tool to use, so all you have to do is run this command if you're in a Linux or Mac OS machine. And if you're in a Windows machine, copy this command and run it on your PowerShell terminal. So after you've done that, we're going to create a folder called basic MCP server. And then I'm going to see the into this folder.
[0:01:00]

[0:01:00]
Then I'm going to run UV init, which will set up a pyproject.toml file inside of that folder, so we have the project set up. And then I'm going to run UV VEMF, which will set up a virtual environment inside of that particular folder. Once I've done that, all I got to do is run source.VEMF bin activate, which will activate the virtual environment. And now I just have to add the necessary dependencies for this particular project. All right, so I'm going to run UV add. And for this particular project, I'm going to be using anthropic as the large language model of choice. And so I'm going to run that, but I'm also going to install the necessary packages for the MCP side. So I'm going to just copy the command from the documentation that we have over here. MCP CLI and HTTPX. So I'm going to control C this. I'm going to control V here. So I have HTTPX, MCP CLI and anthropic. So these are all the dependencies that we need.
[0:02:00]

[0:02:00]
So I'm going to run enter. And now here we go. We have all the dependencies installed. This is beautiful. Now all we need to do is to create the files that we need. So I'm going to run touch MCP server.py. And I'm going to run touch host client test.py. All right. These are the two files that we're going to need, a simple MCP server and then a simple host client.py file. Now I'm going to open up my editor of choice inside of this folder. I use cursor so in the term I'm going to run cursor dot so that it opens cursor inside of this particular folder. Beautiful. And now let me make this very big. I'm here inside of my editor of choice. And what we're going to be doing is I'm going to click on MCP server. And inside of this file we're going to write the necessary code for our MCP server. Now it's very simple to get started. All we're going to do is we're going to say from MCP dot server dot fast MCP.
[0:03:00]

[0:03:00]
We're going to import fast MCP to set up a very simple model MCP server instance that we can just name MCP. I'm going to create a variable called MCP. And then I'm going to set up fast MCP. And I'm going to give this application a name and it's going to be called simple MCP server. All right. Now once we've done that we need to write the functions and the resources and whatever we want. That will be a part of the server. So for that what I'm going to do is I'm going to create a decorator here and I'm going to run MCP dot tool. And now I'm going to write a function that I will be decorating as a tool so exposing as a tool with my MCP server. So I'm going to write death create file and what this function is going to do is very simple. It's going to accept a file path in a file content.
[0:04:00]

[0:04:00]
And what it's going to do is to create a file in that path with the content given. That's very simple, very easy. So I'm going to run file path and then content of the file. And as a I'm going to define that the what is returned by this function is going to be a very simple dictionary containing one key and one value both as strings. Now inside I'm going to write a very simple doc for this function. And curse is already completing everything for me so I'm just going to tab this I don't want the read file that we got here so I'm going to delete this. And for now I think that this is pretty good. What I would like to do for this particular example is I want to return the file path and I want to return the contents of the file. So here's what we're going to do I'm going to put this as a variable that we're going to create here so I don't need to do this I can do it like file path.
[0:05:00]

[0:05:00]
And then here I'm going to put it as content beautiful. So we have to write it like this file path and content beautiful. Now we have an mcp2 that we are exposing. So in order to make this absolutely awesome we're going to say if name equal main mcp.run. And I don't have to write this but I'm going to write transport equals to stdsdio because I'm choosing the transport mechanism to be local standard input output for the communication protocol that we're using, which means that this is going to run locally on my machine. So now that we have the server running I would like to know whether or not it's actually working. So what I'm going to do is I'm going to use a very cool tool that the mcp package already brings preinstalled which is called the mcp inspector.
[0:06:00]

[0:06:00]
So I'm going to open up a terminal like I have over here I'm going to deactivate this default environment I have over here. There you go. And I'm going to run I need to make sure that my virtual environment is actually running on this terminal that I'm interacting with here. So I'm going to run source of them been activate beautiful. And now I'm going to run mcp uv run mcp dev and I'm going to point to the server that we're going to be inspecting. So now it's asking me if I want to install this package. Yes, this is for the application that we're actually going to run. We're going to say yes, we only have to do this once. I'm going to click on this local host that already loads up the application of the mcp inspector with everything ready so that I can connect to my mcp server.
[0:07:00]

[0:07:00]
And this is the mcp inspector which is a really handy to for debugging your mcp servers so you have the transport type the command needed to run the server the arguments environment variables in case you need them. And then I have this little button called connect that I'm going to click and now I'm connected to my mcp server. So what's cool is that when you're exposing tools and prompts and resources you can use the mcp inspector to make sure and look through all of the things that you are exposing via that mcp server. So I'm going to click on tools and then I'm going to list the tools that I'm exposing with the server. So when I click on list tools you can see that I'm listing the create file tool. And if I click on that create file tool we can actually test that tool with a sample file in a sample content. So I'm going to say file.txt. I'm going to say file test test.txt and the content is testing our first mcp server.
[0:08:00]

[0:08:00]
And I'm going to click on run tool and now we can see that the tool result is success and we can see what the tool is returning when we run this tool here. And we can see that the return is file test.txt and the out in the string testing our first mcp server. And if I go back to my editor I can see that if I go here on my left we have a file test file that was created and the contents are correct. So this is awesome. So that means that this server works and it allows me to create files locally on my computer. Perfect. Now if I go back to this mcp server I could if I wanted to ping this server for example I can click on ping and we can see the history here on the bottom. We can do all sorts of things to inspect and explore this mcp inspector tool to interact with our mcp server. But for now this is good enough. So what I'm going to do is I'm going to disconnect to my server. I'm going to delete this. I'm going to go back to our editor here.
[0:09:00]

[0:09:00]
I'm going to stop running this for now and I'm very happy with this server as it is. So I'm going to move to the host client test. Now what we're going to do here folks is we're going to set up a simple simple code that allows us to test this server as if we were the host client application. We're going to simulate a very simple app where we can have some sort of user interaction. And then we are going to simulate how the client would behave to interact with the server. And the same thing we're going to simulate how we would have the host app and a simple client that handles the interactions with the server just like we talked about during the communication flow lesson. So to refresh your memory, let's quickly go over the communication flow protocol that's going to occur right now. We're going to have a user interaction sending a query.
[0:10:00]

[0:10:00]
The host processes that query. The client invokes the capabilities. Those capabilities go back. And then the LLM is going to be able to make invocations to the different tools. In our case is going to be the create file tool. Those are going to be executed. The server is going to run and then it's going to relay the results back to the client, which is going to relay the results back to the host. So our response can be sent back to the user. Alright, so let's simulate that via writing an actual host simple client test file. So instead of writing the whole thing myself, because I want to be a little bit lazy, here's what we're going to do. I'm going to put my editor here on my left, and I'm going to open up my browser here on my right. And I'm going to open up the documentation for client developers in the model context protocol documentation. So I'm going to come over here on the sidebar. I'm going to go over to client developers.
[0:11:00]

[0:11:00]
And don't worry, I'm going to leave the links and all the materials for this lesson, including the code that we write into the GitHub repo that's going to be made available to you as the server. So don't worry about that. And in this section, the model context protocol documentation already provides all that we need to test the simple server that we set up. So I'm going to go down over here. I don't need to go through this process of setting up the project again, because I've already done this when I was setting up the server. Now, obviously, there's something that can be said about setting up the server with one project, with one set of packages and a client with another. But right now we're going to put everything inside of the same project. And I'm not going to be using any environment variable. So we're not going to be using Python dot M, at least now right now, because it's going to be a very simple example. All I want from this documentation is a simple code that allows us to test the basic client structure as you see over here.
[0:12:00]

[0:12:00]
And what I'm going to do is I'm going to copy all of the code that is in this little project in this file. So let me just take this example here. You have a client user interface, so we're going to copy the entire logic right over here. I already have this copied on my computer. So I'm going to do a little bit of a cheat code. And I'm going to run a little command. There you go. And I'm going to go over to the editor. I'm going to paste that code in there. And now we're going to walk through what actually we paste it over here and how this code works. All right. So here's what we're going to do. We have here the imports for the project. All right. The actually don't mind what I said before.
[0:13:00]

[0:13:00]
We will use Python dot M to actually set up the entropic API key. So I'm going to open up my terminal real quick, I'm going to go to inside of this folder. I'm going to run UV add Python dot M, which should set up Python dot M in this project. Beautiful. And now that I've done that, I'm going to go back in here. And these are the imports for the project. All right. So let's walk through what is going on on each of these. And what we're doing here is we're setting up a little class called MCP client. That has all the client structure needed to interact with different servers. All right. So we have a few functions that are worth mentioning. We have the function that connects to the server. A function that processes the query. And we have a function that simulates a chat loop like a chat interface with the user. To simulate that aspect of the host application where the user actually interacts with a large
[0:14:00]

[0:14:00]
language model in some simple chat interface. Just like in cursor, if I open up here on my right, I can have here a little chat interface where I can say hi, cursor, how's it going? All right. So that being said, we have these three functions and then we have a main function that runs this entire loop. Okay. Now, what is going on? We have a function that connects to the server. This function is async, which means that it can be run in asynchronously, so it can run in parallel. And all this function takes is the path to the script of our server, which in our case is MCP server.py, which is in the same folder that host client test.py. And what we're going to do is the in this function, we're just setting up a very simple if statement to see if this is a Python or JS file. We're just going to be using Python for this project, but since I'm copying from the model context documentation, we're going to leave that in.
[0:15:00]

[0:15:00]
And then we're going to, based on whether or not we use Python or not, we're going to set up the initialization parameters. All right. And these are the commands and arguments that we need to run the server. Remember that we could run the server in the terminal by doing this. You've run and then MCP server. So now folks, when I do this, I'm running this server right now, right? I'm exposing this, but now I'm going to stop interacting with this. I'm going to interrupt this running of the server. There you go. However, you can see that all we need to run the server is UV run MCP server.py, right? So this is what's going to actually go over here. The command with the arguments and the path to the script. And also environment variables in case we need any environment variables, right? Now, for the transport, we're going to be loading from enter async context, SDDAO client with the server parameters.
[0:16:00]

[0:16:00]
Now, what's going on over here, folks, is that if we look through the docs on these three pages, what they say in documentation is this. In the server connection management, we're implementing the method to connect to the server, right? And all of the, all of this part is just setting up connection to the server. So that the client can have a way to initialize and connect to that particular server. Then we implement the logic for processing the query, which should be a little bit easier to understand if you've interacted with LLM APIs in the past. We're going to be writing a message to some model that returns a response and then we put that response into the context of the model and so on and so forth. So it's just a core functionality for processing queries and handling calls of tools. Two calls. In this case, the only tool that is available to call is the tool that we've set up in the server, which is called create file, all right? Now, what is this function actually doing in the process query?
[0:17:00]

[0:17:00]
It's saying, okay, so we get a query from the user. This is the query. And then we list the available tools, right? Remember the part in the communication flow about listing and capability discovery. So we're saying, what are the tools available? And then we're organizing what is available into a little list with the name, the description, and the input schema for all the tools that are available. In our case, it's just the create file tool, but that doesn't matter. It could be 30 functions and it would all of those tools would be organized over here. And then we make a call to the Cloud API, so we're making a simple LLM call to the cloud model. In this case, it's the 3.5 Sonnet model, and we set a max number of tokens for 1,000. We send that message list, that messages list that contains the query from the user, and
[0:18:00]

[0:18:00]
the list with dictionary containing the name, description, and the input schema for all the tools, all of that goes into the Cloud model, to the anthropic API client. So then what happens? Well, now we send the request to the, we're processing the request, right? If we remember what we've talked about in the communication flow video, if I go back to that diagram real quick, just so that you can take a look, we are right now in this phase over here, folks, we are understanding the request, all right? So this is the part that we are actually doing right now. So we, let me just actually put this on the left, and then have you see the corresponding section here on the right, okay? Now, we are processing the request. Remember, we already listed the available tools over here, right? Which is the same thing in the client connection to do this part, which is number 4 over here, and I know the order can be a bit tricky, but imagine that this is just a process of just
[0:19:00]

[0:19:00]
describing what happens there, but because this is a synchronous, it's not necessarily in the exact same order. What matters is that we are doing capability discovery, and then we are processing the request, all right? Then now we're going to see what happens. So we're back here, we're processing the request, and here's the thing, folks. Well, the answer from the model can be text, or it can be a tool call, which is identified here with a type called tool use. So this, if the response from the Cloud API indicates tool use, that means that the LLM indicated that it wants to make a invocation of a capability. So it's this part over here that we've talked about, okay? So it's this part over here. We're invoking a specific capability, and then what is happening is that we are now going
[0:20:00]

[0:20:00]
to collect, let me just, I just put the number 7 over there for no reason, there you go. And now we get the name of the tool that we're going to use, which in the case of our example, it's going to be create file, okay? And then we're going to get the inputs that will have been organized by the LLM for using this particular tool, and then we're going to execute that tool, right? And you see that the execution is asynchronous on the client side, so that's why we have execute request functionality here coming from the client to the server, which is what you see here on my right in the diagram, execute request functionality, okay? Then we're going to append the outputs of that execution to this final text list that contains all the messages that are being exchanged during this process of collecting everything that's going on until we get a response. So we're saying calling this tool with this two name with these arguments.
[0:21:00]

[0:21:00]
We're indicating that the tool is being called, right? And obviously this is asynchronous, so we're seeing what happens when we have the result. And now here's the thing, if this content response from the cloud call, from the cloud API call, has this text attribute, and this is not like a nun or something like that, we just return, we just append that response, okay? So this is essentially saying we're either processing text or we're processing a tool call, and then when we process a tool call, we're going to one way or another append that to the messages of the model, okay? Then we're going to get another response from the model, and we're going to append everything that happened into the final text with all of the context that's going to be returned for the user.
[0:22:00]

[0:22:00]
So this is the part of processing the query. Now the final, the final step is actually the chat loop, all right, folks? So what's going on here? This is just a simple loop that's going to run so that the user can send a bunch of requests to this LLM. So the user can be interacting with this host, this fake host with the simple client and server implementation until the user calls quits. So it's a very simple getting some input. The user sends some input to the model to be processed. You see calling the process query function that we just went through, and then it cleans up everything from cache, all right? To run this is very simple. All we have to do is run Python, and then we're actually going to use UV, right? So UV run this client, and we're going to indicate the path to the server. That's all we need. And what's going to happen is initialization of the client instance, we're going to connect to the server, and we're going to run the function chat loop that contains the processing
[0:23:00]

[0:23:00]
of the query right over here, all right? So let's see how that works in practice. I'm going to open up the terminal here on my left, and because we don't need anything about the diagram anymore, I'm going to just maximize this. And here's where we're going to run UV run, host client, and now, uh, UV run, that's my bad, UV run, Python, host client, there you go. Sorry folks, we have to run UV run, host client, and now we've got to give the path to the server, so MCP server. There we go. So now we are connected to the server, and the server contains the tools create file, all right? So here's the thing, I'm going to send a request to the LLM for a simple execution of a creation of a file. So I'm going to say write a simple essay with two paragraphs explaining why pancakes are the best breakfast of all time.
[0:24:00]

[0:24:00]
So now that request is being processed, as we can see here, and there you go. The model is executing a tool, so call to request. And then this file, this essay was created. The file path is pancake essay dot txt, and the contents is this huge, very silly essay about pancakes and so on, all right? And now we're happy with this, so we're going to call quits. And now folks, I'm going to open up the file to see if it worked. And as you can see, it worked beautifully. Pancake stand as the undisputed champion breakfast foods, blah, blah, blah, blah. So as you can see, folks, in a very simple, true script approach, we've interacted with set up the server, and we set up a simple host client example. I mean, we can call this just the client, folks, and some people use it interchangeably,
[0:25:00]

[0:25:00]
but the idea is that the host represents the big user-facing application. And the client is going to be a component inside of the host that handles the MCP server. And we just did all of that, completing the entire communication flow with just a couple of scripts, all right? So in the next videos, we're going to continue to explore and go deeper into MCP. So see you there.